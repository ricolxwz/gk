---
title: 信息论:不确定性和熵
comments: true
---

## 信息

信息包含在问题和答案中. 信息是指一个变量(如答案, 信号, 测量值)减少我们对另一个变量的不确定性或者让我们感到惊讶的程度. 我们需要量化两个方面: 不确定性(熵, entropy)的程度和不确定性减少/让我们惊讶的程度(信息, information). 这些概念可以由香农(Claude Shannon)理论进行量化. 

## 信息论

信息论是一种定量捕捉信息概念的方法. 在传统意义上, 信息论回答了两个问题: 

- 什么是数据的最终压缩: 也就是能将文件压缩到多小
- 什么是最终的通信传输速率: 也就是最高的下载速度是多少

信息论在其他领域也有广泛的应用: 

![](https://img.ricolxwz.io/2024/07/ee100df97b4ef95d182b80b3439e5949.webp){:style="width:400px"}

## 测量单位

信息通过比特为单位来测量. 1比特表示对一个等概率的是/否问题的不确定性的量度, 换句话说, 一个比特可以表示为一个二元决策中的不确定性, 比如一个抛硬币的问题(正面或者反面). 回答这个是/否问题会较少1比特的不确定性或者提供1比特的信息. 例如, 如果你知道一个硬币会是正面还是反面, 你就获得了1比特的信息, 因为你解决了一个二元问题的不确定性. 

## 量化的基本概念

复习一下概率论中的内容, 详情可以参考[这里](https://ml.ricolxwz.de/%E6%A6%82%E7%8E%87/%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/).

- 随机变量: $X$是一个随机变量, 这是一个取值受随机影响的变量. 例如, 一个答案/信号/测量值. 举例来说, 抛硬币的结果, 今天是否下雨的结果都是随机变量
- 样本或结果: $x$是$X$的一个样本或结果, 从一个离散的字母表$A_X=\{x_1, x_2, ...\}$. 对于二元随机变量$X$, 字母表$A_X=\{0, 1\}$; 对于抛硬币的结果, 字母表$A_X=\{heads, tails\}$, 在"猜猜谁?"游戏中, 头发颜色的字母表可以是各种头发的颜色
- 概率分布函数(Probability Distribution Function, PDF): 定义概率分布函数$p(x)$, $p(x)=P(X=x), x\in A_X$. 满足$0\leq p(x)\leq 1$, $\sum_{x\in A_X}p(x)=1, x\in A_X$