---
title: 信息论:估计器
comments: true
---

估计器, Estimator是一种用于从观测数据中推断未知参数或者数量的统计工具. 在机器学习中, "估计器"一词广泛用于指代模型和算法, 像线性回归, 支持向量机, 神经网络等模型都是不同类型的估计器. 估计器是一个更广义的词语, 它可以指代用于分类, 回归或者其他类型的模型; 而分类器是一种估计器的特殊形式, 专门用于分类任务, 输出的是离散的标签. 估计器的一些可能的特征包括: 

- 无偏性: 若估计器的期望等于真实参数值, 则称估计器是无偏的. 即当对一个无偏估计器进行反复独立采样的时候, 长期来看, 得到的估计值的平均值应该等于真实参数
- 一致性: 若随着样本量的增加, 估计值收敛到真实参数值, 则称估计器是一致的
- 效率: 在无偏估计器中, 方差最小的估计器被认为是最有效的
- 充分性: 若估计器中包含了样本中关于未知参数的所有信息, 则称其为充分统计量

???+ warning "注意"

    无偏性不能保证一致性: 一个无偏估计器的期望值等于真实参数值, 但是这并不能保证每次采样的估计值都等于真实值. 假设我们有一个无偏估计器, 其期望值等于真实值, 但是它的方差不随着采样的样本量增加而减少, 那么即使在较大样本量的情况下, 估计器的估计值可能仍然会和真实值有很大偏差, 因此, 这种估计器是无偏的, 但是它并不具有一致性. 如果一个估计器是一致的, 意味着当样本量增加的时候, 估计值不仅期望需要接近真实值, 即需要接近于是一个无偏估计器, 还需要其方差收敛到$0$. 

## 方差和偏差

当我们计算从经验值计算信息论的测量值如熵的时候, 我们计算的其实是真实值$H$的估计$\hat{H}$.

如投硬币, 我们投$6$次是正面, $4$次是反面, 计算得到的估计熵是$-\frac{2}{5}\log_2\frac{2}{5}-\frac{3}{5}\log_2\frac{3}{5}$. 但是真实值是$-\frac{1}{2}\log_2\frac{1}{2}-\frac{1}{2}\log_2\frac{1}{2}$. 在这里, 我们的估计器是熵的计算方法. 当我们进行多次这样的独立的投硬币实验之后(每组10次), 我们就会得到一个估计熵的平均值, 即$<\hat{H}>$, 然后, 我们就可以计算偏差, 即$B=<\hat{H}>-H$. 还可以得到方差$v(\hat{H})$.

在数据量有限的时候, 估计器的偏差和方差会更加显著. 如熵在有限数据下往往会被低估, 即观测到的熵值通常比真实的熵值要小; 互信息往往会被高估, 即观测到的互信息通常比真实值要大.