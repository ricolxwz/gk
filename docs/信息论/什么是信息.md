---
title: 信息论:什么是信息
comments: true
---

## KL散度 [^1]

相对熵, 又称为KL散度. 如果我们对于同一个随机变量$X$有两个单独的概率分布$P(X)$和$Q(X)$(我们通常用$P$来表示真实的概率分布, $Q$来表示模型所预测的概率分布), 我们可以使用KL散度(Kullback-Leibler Divergence)来衡量这两个分布之间的差异. 在机器学习中, 我们需要衡量标签和预测值之间的差距, 用KL散度就刚刚好, 即$D(y||\hat{y})$.

KL散度的计算公式为: $D(p||q)=\sum_{i=1}^n p(x_i)\log_2 \frac{p(x_i)}{q(x_i)}$, 其中$n$为事件的所有可能性. KL散度的值越小, 表示$Q$分布和$P$分布越接近. 当KL散度等于$0$的时候, 意味着概率分布$Q$和概率分布$P$是相同的.

## 交叉熵 [^1]

对与上述KL散度的计算公式进行变形:

$D(p||q)=\sum_{i=1}^n p(x_i)\log_2 \frac{p(x_i)}{q(x_i)} = \sum_{i=1}^n p(x_i)\log_2 p(x_i)-\sum_{i=1}^np(x_i)\log_2 q(x_i)=-H(p(x))+[-\sum_{i=1}^n p(x_i)\log_2 q(x_i)]$

可以观察到等式的前一部分恰好就是$p$的熵的负数$-H(p(x))$, 等式的后一部分, 就是交叉熵: $G(p||q)=-\sum_{i=1}^n p(x_i)\log_2 q(x_i)$. 由于在优化的过程中, 真实的概率分布永远是不变的, 即$-H(p(x))$永远保持不变, 所以一般只需要关注交叉熵就可以了.

也就是说, 交叉熵和KL散度衡量的都是你使用一个错误的概率分布的时候付出的"额外代价". KL散度/交叉熵绝对值越大, 说明必须额外传输更多的信息才能进行正确编码数据, 因为你得到的概率分布和实际的分布不符.

## 互信息 [^2]

互信息, Mutual Information. 是描述一个随机变量中包含的关于另一个随机变量的信息. 用Venn图表示如下(注意中间的那个部分):

![](https://img.ricolxwz.io/2024/08/9cd4d2a8fc2d099426208f752376e8b7.png)

从Venn图中, 可以看出:

- $I(X;Y)=H(X)+H(Y)-H(X,Y)$
- $I(X;Y)=H(X)-H(X|Y)$
- $I(X;Y)=H(Y)-H(Y|X)$
- $I(X;Y)=I(Y;X)$

有如下属性:

- $0\leq I(X;Y)\leq min(H(X), H(Y))$
- 在$X$和$Y$中是对称的
- $I(X;Y)=H(X)\rightarrow H(X|Y)=0$
- 是非线性的

???+ example "例子"

    可以用赌博来解释互信息.

    如果你对比赛的结果没有任何额外的信息, 那么最好的策略就是根据结果$X=x$的概率$p(x)$来分配你的赌注. 例如, 如果某个结果的$x$的概率更高, 那么你应该在这个结果中投入更多的资金. 如果有额外的信息$Y$, 最佳策略是根据$p(X|Y)$分配投资, 为什么? 因为如果你有额外的信息$Y$, 并且这个$Y$和$X$有关系, 即$Y$中包含了$X$的一些信息, 互信息大于$0$. 换句话说, 互信息 $I(X;Y)$量化了利用额外信息$Y$进行投资相比于没有额外信息时的优势.

### 自互信息

一个随机变量和自己的互信息就是它的熵.

$I(X;X)=H(X)+H(X)-H(X,X)$, 可以简化为$I(X;X)=H(X)$. 这种情况下, 互信息测量的就是变量$X$对自身提供的信息量. 因为$X$完全了解自己, 所以它对自身的自互信息就等于它的熵$H(X)$. 即这个随机变量包含这个随机变量的信息就是它自己包含的信息.

![](https://img.ricolxwz.io/2024/08/944041a23df4e876dd8050b45aed57ca.png)

### 交叉熵和互信息的关系

来看两个随机变量, 对于两个随机变量$X$和$Y$, 我们要探究这两个随机变量之间的关联关系, 即这两个随机变量是否是独立的, 即在Venn图中是否有相交的部分, 我们知道, 这个相交的程度就是互信息. 我们将会尝试将KL散度的那套理论搬到这里, 真实的概率分布$P$就是真实情况下的联合分布, 现在假设这两个变量之间是独立的, 即$p(x, y)=p(x)p(y)$. 所以交叉熵$D(p(x, y)||p(x)p(y))=\sum_{x\in A_x, y\in A_y}p(x, y)\log_2 \frac{p(x, y)}{p(x)p(y)}$. 套用贝叶斯公式, $\frac{p(x|y)}{p(x)}=\frac{p(x, y)}{p(x)p(y)}$, 可以得到交叉熵$D(p(x, y)||p(x)p(y))=\sum_{x\in A_x, y\in A_y}p(x, y)\log_2\frac{p(x|y)}{p(x)}$. 而这个交叉熵就是我们要求的互信息, 即$I(X; Y)=D(p(x, y)||p(x)p(y))$, 因为根据交叉熵的定义: 交叉熵衡量的是一个错误的概率分布的时候付出的"额外代价", 我们的错误的概率分布假设的是它们之间没有关系, 所以计算出来的交叉熵就是那部分"额外代价", 即互信息. 若这两个变量之间没有关系, "额外代价"等于$0$, 互信息等于$0$, 这两个量之间是独立的. 

### 某个随机变量结果的互信息

上面我们讨论的是一个两个随机变量的互信息, 而现在我们要讨论的是两个随机变量结果的互信息, 用$i(x;y)$表示. 它的含义是某个随机变量的结果中包含的关于另一个随机变量的结果的信息. 

它的属性和随机变量的差不多:

- $i(x;y)=h(x)+h(y)-h(x,y)$
- $i(x;y)=h(x)-h(x|y)$
- $i(x;y)=h(y)-h(y|x)$
- $i(x;y)=\log_2 \frac{p(x|y)}{p(x)}$
- $I(X;Y)=<i(x;y)>$

随机变量的互信息一定大于等于$0$, 但是某个随机变量结果的互信息可以大于$0$, 也可以小于$0$:

- $i(x;y)>0$说明$y$的发生增加了$x$发生的可能
- $i(x;y)<0$说明$y$的发生减小了$x$发生的可能
- $i(x;y)=0$说明$y$的发生和$x$没有关系, 没有增加也没有减少$x$发生的可能

### 条件互信息

条件互信息$I(X;Y|Z)$是给出另一个随机变量$Z$的分布的前提下, 一个随机变量包含另一个随机变量的信息.

### 应用

互信息可以用于:

- 变量之间的关系检测
- 机器学习中的特征选择, 比如决策树, 互信息和信息增量是一个概念
- ...

[^1]: 一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉-CSDN博客. (n.d.). Retrieved August 14, 2024, from https://blog.csdn.net/tsyccnh/article/details/79163834
[^2]: 什么是互信息Mutual information-CSDN博客. (n.d.). Retrieved August 14, 2024, from https://blog.csdn.net/qq_40210586/article/details/131699236