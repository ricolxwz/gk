---
title: 信息论:什么是信息
comments: true
---

## KL散度 [^1]

相对熵, 又称为KL散度. 如果我们对于同一个随机变量$X$有两个单独的概率分布$P(X)$和$Q(X)$(我们通常用$P$来表示真实的概率分布, $Q$来表示模型所预测的概率分布), 我们可以使用KL散度(Kullback-Leibler Divergence)来衡量这两个分布之间的差异. 在机器学习中, 我们需要衡量标签和预测值之间的差距, 用KL散度就刚刚好, 即$D(y||\hat{y})$.

KL散度的计算公式为: $D(p||q)=\sum_{i=1}^n p(x_i)\log_2 \frac{p(x_i)}{q(x_i)}$, 其中$n$为事件的所有可能性. KL散度的值越小, 表示$Q$分布和$P$分布越接近. 当KL散度等于$0$的时候, 意味着概率分布$Q$和概率分布$P$是相同的.

## 交叉熵 [^1]

对与上述KL散度的计算公式进行变形:

$D(p||q)=\sum_{i=1}^n p(x_i)\log_2 \frac{p(x_i)}{q(x_i)} = \sum_{i=1}^n p(x_i)\log_2 p(x_i)-\sum_{i=1}^np(x_i)\log_2 q(x_i)=-H(p(x))+[-\sum_{i=1}^n p(x_i)\log_2 q(x_i)]$

可以观察到等式的前一部分恰好就是$p$的熵的负数$-H(p(x))$, 等式的后一部分, 就是交叉熵: $G(p||q)=-\sum_{i=1}^n p(x_i)\log_2 q(x_i)$. 由于在优化的过程中, 真实的概率分布永远是不变的, 即$-H(p(x))$永远保持不变, 所以一般只需要关注交叉熵就可以了.

也就是说, 交叉熵和KL散度衡量的都是你使用一个错误的概率分布的时候付出的"额外代价". KL散度/交叉熵绝对值越大, 说明必须额外传输更多的信息才能进行正确编码数据, 因为你得到的概率分布和实际的分布不符.

## 互信息 [^2]

互信息, Mutual Information. 是描述一个随机变量中包含的关于另一个随机变量的信息. 用Venn图表示如下(注意中间的那个部分):

![](https://img.ricolxwz.io/2024/08/9cd4d2a8fc2d099426208f752376e8b7.png)

从Venn图中, 可以看出:

- $I(X;Y)=H(X)+H(Y)-H(X,Y)$
- $I(X;Y)=H(X)-H(X|Y)$
- $I(X;Y)=H(Y)-H(Y|X)$
- $I(X;Y)=I(Y;X)$

有如下属性:

- $0\leq I(X;Y)\leq min(H(X), H(Y))$
- 在$X$和$Y$中是对称的
- $I(X;Y)=H(X)\rightarrow H(X|Y)=0$

[^1]: 一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉-CSDN博客. (n.d.). Retrieved August 14, 2024, from https://blog.csdn.net/tsyccnh/article/details/79163834
[^2]: 什么是互信息Mutual information-CSDN博客. (n.d.). Retrieved August 14, 2024, from https://blog.csdn.net/qq_40210586/article/details/131699236