---
title: 信息论:什么是信息
comments: true
---

## KL散度

相对熵, 又称为KL散度. 如果我们对于同一个随机变量$X$有两个单独的概率分布$P(X)$和$Q(X)$(我们通常用$P$来表示真实的概率分布, $Q$来表示模型所预测的概率分布), 我们可以使用KL散度(Kullback-Leibler Divergence)来衡量这两个分布之间的差异. 在机器学习中, 我们需要衡量标签和预测值之间的差距, 用KL散度就刚刚好, 即$D(y||\hat{y})$.

KL散度的计算公式为: $D(p||q)=\sum_{i=1}^n p(x_i)\log_2 \frac{p(x_i)}{q(x_i)}$, 其中$n$为事件的所有可能性. KL散度的值越小, 表示$Q$分布和$P$分布越接近. 当KL散度等于$0$的时候, 意味着概率分布$Q$和概率分布$P$是相同的.

## 交叉熵

对与上述KL散度的计算公式进行变形:

$D(p||q)=\sum_{i=1}^n p(x_i)\log_2 \frac{p(x_i)}{q(x_i)} = \sum_{i=1}^n p(x_i)\log_2 p(x_i)-\sum_{i=1}^np(x_i)\log_2 q(x_i)=-H(p(x))+[-\sum_{i=1}^n p(x_i)\log_2 q(x_i)]$

可以观察到等式的前一部分恰好就是$p$的熵的负数$-H(p(x))$, 等式的后一部分, 就是交叉熵: $G(p||q)=-\sum_{i=1}^n p(x_i)\log_2 q(x_i)$. 由于在优化的过程中, 真实的概率分布永远是不变的, 即$-H(p(x))$永远保持不变, 所以一般只需要关注交叉熵就可以了.

也就是说, 交叉熵和KL散度衡量的都是你使用一个错误的概率分布的时候付出的"额外代价". KL散度/交叉熵绝对值越大, 说明必须额外传输更多的信息才能进行正确编码数据, 因为你得到的概率分布和实际的分布不符.