{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infocontent(p):\n",
    "    \n",
    "    # Alter the equation below to provide the correct Shannon information \n",
    "    # content:\n",
    "\n",
    "    return -np.log2(p)\n",
    "\n",
    "def entropy(p):  \n",
    "    # First make sure the array is now a numpy array\n",
    "    if type(p) != np.array:\n",
    "        p = np.array(p)\n",
    "\n",
    "    # Should we check any potential error conditions on the input?\n",
    "    if (abs(np.sum(p) - 1) > 0.00001):\n",
    "        raise Exception(\"Probability distribution must sum to 1: sum is %.4f\" % np.sum(p))\n",
    "    \n",
    "    # We need to take the expectation value over the Shannon info content at\n",
    "    # p(x) for each outcome x:\n",
    "    weightedShannonInfos = p*(infocontent(p))\n",
    "    # nansum ignores the nans from calling infocontent(0), but we still get the warning if an entry in p is zero\n",
    "    return np.nansum(weightedShannonInfos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" function jointentropy(p)\n",
    "Computes the joint Shannon entropy over all outcome vectors x of a vector\n",
    "random variable X with probability matrix p(x) for each candidate outcome\n",
    "vector x.\n",
    "\n",
    "Inputs:\n",
    "- p - probability distribution function over all outcome vectors x.\n",
    "   p is a matrix over all combinations of the sub-variables of x,\n",
    "where p(1,3) gives the probability of the first symbol of sub-variable\n",
    "x1 co-occuring with the third symbol of sub-variable x2.\n",
    "   E.g. p = [0.2, 0.3; 0.1, 0.4]. The sum over p must be 1.\n",
    "\n",
    "Outputs:\n",
    "- result - joint Shannon entropy of the probability distribution p\n",
    "\n",
    "Copyright (C) 2020-, Julio Correa, Joseph T. Lizier\n",
    "Distributed under GNU General Public License v3\n",
    "\"\"\"\n",
    "def jointentropy(p):\n",
    "    \n",
    "\t# Should we check any potential error conditions on the input?\n",
    "\n",
    "\t# We need to take the expectation value over the Shannon info content at\n",
    "\t#  p(x) for each outcome x in the joint PDF:\n",
    "\t# Hint: will your code for entropy(p) work, or can you alter it slightly\n",
    "\t#  to make it work?\n",
    "    \n",
    "    joint_entropy = entropy(p)\n",
    "    \n",
    "    return joint_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
